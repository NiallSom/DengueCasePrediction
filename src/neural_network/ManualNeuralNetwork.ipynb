{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>...</td>\n",
       "      <td>32.00</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>...</td>\n",
       "      <td>17.94</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>...</td>\n",
       "      <td>26.10</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>...</td>\n",
       "      <td>13.90</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.20</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>0.342750</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.256343</td>\n",
       "      <td>0.292514</td>\n",
       "      <td>55.30</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>88.765714</td>\n",
       "      <td>55.30</td>\n",
       "      <td>18.485714</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>28.633333</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>35.4</td>\n",
       "      <td>22.4</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.160371</td>\n",
       "      <td>0.136043</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>86.47</td>\n",
       "      <td>...</td>\n",
       "      <td>207.10</td>\n",
       "      <td>91.600000</td>\n",
       "      <td>86.47</td>\n",
       "      <td>18.070000</td>\n",
       "      <td>7.471429</td>\n",
       "      <td>27.433333</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>34.7</td>\n",
       "      <td>21.7</td>\n",
       "      <td>36.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>0.247057</td>\n",
       "      <td>0.146057</td>\n",
       "      <td>0.250357</td>\n",
       "      <td>0.233714</td>\n",
       "      <td>58.94</td>\n",
       "      <td>...</td>\n",
       "      <td>50.60</td>\n",
       "      <td>94.280000</td>\n",
       "      <td>58.94</td>\n",
       "      <td>17.008571</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>32.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.245771</td>\n",
       "      <td>0.278886</td>\n",
       "      <td>0.325486</td>\n",
       "      <td>59.67</td>\n",
       "      <td>...</td>\n",
       "      <td>62.33</td>\n",
       "      <td>94.660000</td>\n",
       "      <td>59.67</td>\n",
       "      <td>16.815714</td>\n",
       "      <td>7.871429</td>\n",
       "      <td>25.433333</td>\n",
       "      <td>8.733333</td>\n",
       "      <td>31.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>iq</td>\n",
       "      <td>2010</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>0.298186</td>\n",
       "      <td>0.232971</td>\n",
       "      <td>0.274214</td>\n",
       "      <td>0.315757</td>\n",
       "      <td>63.22</td>\n",
       "      <td>...</td>\n",
       "      <td>36.90</td>\n",
       "      <td>89.082857</td>\n",
       "      <td>63.22</td>\n",
       "      <td>17.355714</td>\n",
       "      <td>11.014286</td>\n",
       "      <td>27.475000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>33.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1456 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city  year  weekofyear  total_cases week_start_date   ndvi_ne   ndvi_nw  \\\n",
       "0      sj  1990          18            4      1990-04-30  0.122600  0.103725   \n",
       "1      sj  1990          19            5      1990-05-07  0.169900  0.142175   \n",
       "2      sj  1990          20            4      1990-05-14  0.032250  0.172967   \n",
       "3      sj  1990          21            3      1990-05-21  0.128633  0.245067   \n",
       "4      sj  1990          22            6      1990-05-28  0.196200  0.262200   \n",
       "...   ...   ...         ...          ...             ...       ...       ...   \n",
       "1451   iq  2010          21            5      2010-05-28  0.342750  0.318900   \n",
       "1452   iq  2010          22            8      2010-06-04  0.160157  0.160371   \n",
       "1453   iq  2010          23            1      2010-06-11  0.247057  0.146057   \n",
       "1454   iq  2010          24            1      2010-06-18  0.333914  0.245771   \n",
       "1455   iq  2010          25            4      2010-06-25  0.298186  0.232971   \n",
       "\n",
       "       ndvi_se   ndvi_sw  precipitation_amt_mm  ...  \\\n",
       "0     0.198483  0.177617                 12.42  ...   \n",
       "1     0.162357  0.155486                 22.82  ...   \n",
       "2     0.157200  0.170843                 34.54  ...   \n",
       "3     0.227557  0.235886                 15.36  ...   \n",
       "4     0.251200  0.247340                  7.52  ...   \n",
       "...        ...       ...                   ...  ...   \n",
       "1451  0.256343  0.292514                 55.30  ...   \n",
       "1452  0.136043  0.225657                 86.47  ...   \n",
       "1453  0.250357  0.233714                 58.94  ...   \n",
       "1454  0.278886  0.325486                 59.67  ...   \n",
       "1455  0.274214  0.315757                 63.22  ...   \n",
       "\n",
       "      reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
       "0                               32.00                             73.365714   \n",
       "1                               17.94                             77.368571   \n",
       "2                               26.10                             82.052857   \n",
       "3                               13.90                             80.337143   \n",
       "4                               12.20                             80.460000   \n",
       "...                               ...                                   ...   \n",
       "1451                            45.00                             88.765714   \n",
       "1452                           207.10                             91.600000   \n",
       "1453                            50.60                             94.280000   \n",
       "1454                            62.33                             94.660000   \n",
       "1455                            36.90                             89.082857   \n",
       "\n",
       "      reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
       "0                            12.42                              14.012857   \n",
       "1                            22.82                              15.372857   \n",
       "2                            34.54                              16.848571   \n",
       "3                            15.36                              16.672857   \n",
       "4                             7.52                              17.210000   \n",
       "...                            ...                                    ...   \n",
       "1451                         55.30                              18.485714   \n",
       "1452                         86.47                              18.070000   \n",
       "1453                         58.94                              17.008571   \n",
       "1454                         59.67                              16.815714   \n",
       "1455                         63.22                              17.355714   \n",
       "\n",
       "      reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
       "0              2.628571           25.442857                 6.900000   \n",
       "1              2.371429           26.714286                 6.371429   \n",
       "2              2.300000           26.714286                 6.485714   \n",
       "3              2.428571           27.471429                 6.771429   \n",
       "4              3.014286           28.942857                 9.371429   \n",
       "...                 ...                 ...                      ...   \n",
       "1451           9.800000           28.633333                11.933333   \n",
       "1452           7.471429           27.433333                10.500000   \n",
       "1453           7.500000           24.400000                 6.900000   \n",
       "1454           7.871429           25.433333                 8.733333   \n",
       "1455          11.014286           27.475000                 9.900000   \n",
       "\n",
       "      station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
       "0                   29.4                20.0               16.0  \n",
       "1                   31.7                22.2                8.6  \n",
       "2                   32.2                22.8               41.4  \n",
       "3                   33.3                23.3                4.0  \n",
       "4                   35.0                23.9                5.8  \n",
       "...                  ...                 ...                ...  \n",
       "1451                35.4                22.4               27.0  \n",
       "1452                34.7                21.7               36.6  \n",
       "1453                32.2                19.2                7.4  \n",
       "1454                31.2                21.0               16.0  \n",
       "1455                33.7                22.2               20.4  \n",
       "\n",
       "[1456 rows x 25 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dengue_data = pd.read_csv(\"data/cleaned_dengue_merged.csv\")\n",
    "dengue_labels = pd.read_csv(\"data/dengue_labels_train.csv\")\n",
    "dengue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['1990-04-30', '1990-05-07', '1990-05-14', ..., '2010-06-11',\n",
       "        '2010-06-18', '2010-06-25'], dtype=object),\n",
       " array([4, 5, 4, ..., 1, 1, 4], dtype=object))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(dengue_data)[:,1:]\n",
    "y_train = np.array(dengue_labels)[:,-1]\n",
    "X_train[:,3], y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(date_str):\n",
    "    year, month, day = map(int, date_str.split('-'))\n",
    "    return year, month, day\n",
    "\n",
    "dates = X_train[:, 3]\n",
    "\n",
    "if type(dates[0]) == str:\n",
    "    years, months, days = zip(*map(split_date, dates))\n",
    "\n",
    "    # Replace the date column with the split components\n",
    "    X_train[:, 3] = years\n",
    "    X_train = np.insert(X_train, 3, months, axis=1)\n",
    "    X_train = np.insert(X_train, 4, days, axis=1)\n",
    "\n",
    "# Now convert the numerical columns to float\n",
    "numerical_columns = [2, 3, 4]\n",
    "X_train[:, numerical_columns] = X_train[:, numerical_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.0, 5.0, 5.0, ..., 6.0, 6.0, 6.0], dtype=object),\n",
       " array([30.0, 7.0, 14.0, ..., 11.0, 18.0, 25.0], dtype=object),\n",
       " array([1990, 1990, 1990, ..., 2010, 2010, 2010], dtype=object),\n",
       " array([0.1226, 0.1699, 0.03225, ..., 0.2470571, 0.3339143, 0.2981857],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,3], X_train[:,4], X_train[:,5], X_train[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 26) (1164, 1)\n",
      "(292, 26) (292, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))  # Reshape y_train before scaling\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "# Print the shapes of training and testing data\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                1728      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,353\n",
      "Trainable params: 4,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of features in your dataset\n",
    "num_features = 26  # Adjust this based on your actual dataset\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Dense(64, activation='softplus', input_shape=(num_features,)))\n",
    "\n",
    "# Add hidden layers\n",
    "model.add(Dense(32, activation='softplus'))\n",
    "model.add(Dense(16, activation='softplus'))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Mean Squared Error loss for regression task\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 26) (1164, 1)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 715us/step - loss: 24099.9961 - mae: 98.5983\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 659us/step - loss: 1284.2712 - mae: 19.8244\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 682us/step - loss: 1139.0029 - mae: 18.0489\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 705us/step - loss: 1013.1793 - mae: 17.2519\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 685us/step - loss: 880.8043 - mae: 15.8010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 702us/step - loss: 725.5252 - mae: 14.1502\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 722us/step - loss: 580.0128 - mae: 13.2127\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 747us/step - loss: 440.8921 - mae: 11.1251\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 732us/step - loss: 293.3674 - mae: 8.7710\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 768us/step - loss: 141.5691 - mae: 6.2239\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 737us/step - loss: 70.2403 - mae: 4.3445\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 750us/step - loss: 26.9060 - mae: 2.8134\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 708us/step - loss: 14.5522 - mae: 2.2730\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 687us/step - loss: 8.2407 - mae: 1.7503\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 696us/step - loss: 5.5756 - mae: 1.5089\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 714us/step - loss: 4.2809 - mae: 1.3256\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 695us/step - loss: 2.6769 - mae: 0.9759\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 681us/step - loss: 2.4109 - mae: 0.9393\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 701us/step - loss: 2.1471 - mae: 0.9049\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 693us/step - loss: 1.6766 - mae: 0.7779\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 746us/step - loss: 1.6660 - mae: 0.7929\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 789us/step - loss: 1.2214 - mae: 0.6543\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 796us/step - loss: 1.0365 - mae: 0.5666\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.8248 - mae: 0.4406\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.7672 - mae: 0.4313\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.7614 - mae: 0.4553\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6808 - mae: 0.4142\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.6875 - mae: 0.4447\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.6830 - mae: 0.4715\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.7558 - mae: 0.5513\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 737us/step - loss: 0.8481 - mae: 0.6369\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 709us/step - loss: 0.5459 - mae: 0.4417\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.5268 - mae: 0.4228\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 687us/step - loss: 0.5707 - mae: 0.4618\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 697us/step - loss: 0.5907 - mae: 0.4657\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 0.4545 - mae: 0.4050\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 0.4008 - mae: 0.3694\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 679us/step - loss: 0.4348 - mae: 0.4283\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 686us/step - loss: 0.4612 - mae: 0.4757\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 680us/step - loss: 0.4881 - mae: 0.4968\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 692us/step - loss: 0.7513 - mae: 0.6703\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 673us/step - loss: 0.3279 - mae: 0.3697\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 677us/step - loss: 0.5965 - mae: 0.5989\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 684us/step - loss: 0.3054 - mae: 0.3690\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 684us/step - loss: 0.2597 - mae: 0.3416\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 666us/step - loss: 0.3741 - mae: 0.4615\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 680us/step - loss: 0.3163 - mae: 0.4081\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 690us/step - loss: 0.1874 - mae: 0.2577\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 0.1961 - mae: 0.2876\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 680us/step - loss: 0.5950 - mae: 0.6019\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 684us/step - loss: 0.3302 - mae: 0.4054\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 680us/step - loss: 0.2821 - mae: 0.4024\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2463 - mae: 0.3695\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.3610 - mae: 0.4584\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 682us/step - loss: 0.2318 - mae: 0.3612\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 683us/step - loss: 0.2784 - mae: 0.4012\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 676us/step - loss: 0.2545 - mae: 0.3956\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 677us/step - loss: 0.4948 - mae: 0.5738\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 0.3169 - mae: 0.4174\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 682us/step - loss: 0.1940 - mae: 0.3245\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 676us/step - loss: 0.3429 - mae: 0.4633\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 674us/step - loss: 0.4917 - mae: 0.5686\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 0.2525 - mae: 0.3962\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 674us/step - loss: 0.2607 - mae: 0.4089\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 686us/step - loss: 0.5733 - mae: 0.6522\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 0.3192 - mae: 0.4574\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 671us/step - loss: 0.2590 - mae: 0.3856\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 0.5569 - mae: 0.6401\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 713us/step - loss: 0.5960 - mae: 0.6337\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 680us/step - loss: 0.3004 - mae: 0.4141\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 692us/step - loss: 0.3749 - mae: 0.4707\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 666us/step - loss: 0.6760 - mae: 0.6781\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 686us/step - loss: 0.9713 - mae: 0.8450\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 685us/step - loss: 0.6612 - mae: 0.7028\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 1.7953 - mae: 1.1724\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 681us/step - loss: 1.3474 - mae: 1.0006\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 701us/step - loss: 0.8088 - mae: 0.7499\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 679us/step - loss: 0.4887 - mae: 0.5907\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 0.9359 - mae: 0.7568\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.2938 - mae: 0.9738\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 813us/step - loss: 1.0712 - mae: 0.8390\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 684us/step - loss: 1.0121 - mae: 0.8074\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 684us/step - loss: 1.3575 - mae: 0.9351\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 2.4662 - mae: 1.3573\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 675us/step - loss: 2.8868 - mae: 1.4512\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 0.4158 - mae: 0.5445\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 658us/step - loss: 1.2043 - mae: 0.8919\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 688us/step - loss: 1.5274 - mae: 1.0332\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 672us/step - loss: 1.1488 - mae: 0.9256\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 691us/step - loss: 1.9026 - mae: 1.1512\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 685us/step - loss: 1.8204 - mae: 1.1444\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 673us/step - loss: 2.0522 - mae: 1.1838\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 675us/step - loss: 2.8161 - mae: 1.3979\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 673us/step - loss: 1.2100 - mae: 0.9312\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 683us/step - loss: 0.2032 - mae: 0.3668\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 676us/step - loss: 3.8183 - mae: 1.7374\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 671us/step - loss: 2.1934 - mae: 1.2034\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 685us/step - loss: 0.7923 - mae: 0.6835\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 678us/step - loss: 2.8356 - mae: 1.4873\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 666us/step - loss: 1.9756 - mae: 1.1816\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "history = model.fit(X_train.astype(float), y_train.astype(float), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 515us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([12.268897], dtype=float32), array([11], dtype=object))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test.astype(float))\n",
    "prediction[20], y_test[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.6100099919593498\n",
      "R-squared (R2) Score: 0.999098706599801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming y_pred and y_test are the predicted and true values respectively\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted target values, respectively\n",
    "r2 = r2_score(y_test, prediction)\n",
    "\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sj', 1990, 18, 4],\n",
       "       ['sj', 1990, 19, 5],\n",
       "       ['sj', 1990, 20, 4],\n",
       "       ...,\n",
       "       ['iq', 2004, 44, 6],\n",
       "       ['iq', 2004, 45, 22],\n",
       "       ['iq', 2004, 46, 37]], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to get weather characteristics from city year and week\n",
    "weather_to_characteristics = np.array(dengue_labels)[:X_train.shape[0],:]\n",
    "weather_to_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels:\n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'labels' is a list or array containing the string labels 'sj' and 'iq'\n",
    "labels = ['sj', 'iq']\n",
    "\n",
    "# Create the OneHotEncoder object\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder to the labels and transform them\n",
    "encoded_labels = encoder.fit_transform(np.array(weather_to_characteristics[:,0]).reshape(-1, 1))\n",
    "\n",
    "# Convert the encoded labels to an array\n",
    "encoded_labels_array = encoded_labels.toarray()\n",
    "\n",
    "print(\"Encoded labels:\\n\", encoded_labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 1990]\n",
      " [0.0 1.0 1990]\n",
      " [0.0 1.0 1990]\n",
      " ...\n",
      " [1.0 0.0 2004]\n",
      " [1.0 0.0 2004]\n",
      " [1.0 0.0 2004]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1164, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_to_characteristics = np.delete(weather_to_characteristics, 0, axis=1)\n",
    "print(weather_to_characteristics)\n",
    "weather_to_characteristics = np.concatenate((encoded_labels_array, weather_to_characteristics), axis=1)\n",
    "weather_to_characteristics = np.delete(weather_to_characteristics, 4, axis=1)\n",
    "weather_to_characteristics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 20)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_to_characteristics_y = X_train[:, 6:]\n",
    "weather_to_characteristics_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 128)               640       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,636\n",
      "Trainable params: 11,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of features in your dataset\n",
    "num_features = 4  # Adjust this based on your actual dataset\n",
    "\n",
    "# Initialize the model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model_1.add(Dense(128, activation='softplus', input_shape=(num_features,)))\n",
    "\n",
    "# Add hidden layers\n",
    "model_1.add(Dense(64, activation='softplus'))\n",
    "model_1.add(Dense(32, activation='softplus'))\n",
    "\n",
    "# Add output layer\n",
    "model_1.add(Dense(20, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Mean Squared Error loss for regression task\n",
    "\n",
    "# Print model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 838us/step - loss: 23053.7266 - mae: 91.6024\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 18064.9941 - mae: 79.7824\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 895us/step - loss: 5029.2925 - mae: 42.2720\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 964us/step - loss: 640.4763 - mae: 14.1388\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 997us/step - loss: 384.7398 - mae: 8.1799\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 890us/step - loss: 377.9340 - mae: 7.4965\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 903us/step - loss: 378.1604 - mae: 7.5939\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 864us/step - loss: 377.6323 - mae: 7.5207\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 858us/step - loss: 379.0117 - mae: 7.5585\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 909us/step - loss: 378.3803 - mae: 7.5974\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 866us/step - loss: 378.3885 - mae: 7.4972\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 939us/step - loss: 378.4784 - mae: 7.5625\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 927us/step - loss: 377.7086 - mae: 7.5365\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 953us/step - loss: 377.5964 - mae: 7.5425\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 378.0147 - mae: 7.5665\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 378.3299 - mae: 7.5433\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 966us/step - loss: 379.0993 - mae: 7.5712\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 982us/step - loss: 377.6477 - mae: 7.5114\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 960us/step - loss: 377.8157 - mae: 7.5904\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 962us/step - loss: 377.8037 - mae: 7.4940\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 378.2794 - mae: 7.5358\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 958us/step - loss: 378.2906 - mae: 7.6078\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 864us/step - loss: 378.1532 - mae: 7.5025\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 880us/step - loss: 378.0238 - mae: 7.5561\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 936us/step - loss: 378.1876 - mae: 7.5282\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 887us/step - loss: 377.6085 - mae: 7.6061\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 863us/step - loss: 378.1105 - mae: 7.5242\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 869us/step - loss: 378.0657 - mae: 7.5321\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 866us/step - loss: 379.1322 - mae: 7.5979\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 918us/step - loss: 379.2247 - mae: 7.5618\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 845us/step - loss: 377.3600 - mae: 7.4969\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 378.6521 - mae: 7.5977\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 378.2021 - mae: 7.5346\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 378.6355 - mae: 7.5538\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 378.0687 - mae: 7.5706\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 845us/step - loss: 377.7076 - mae: 7.5666\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 378.5814 - mae: 7.5322\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 862us/step - loss: 378.1570 - mae: 7.5635\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 911us/step - loss: 378.1049 - mae: 7.4950\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 904us/step - loss: 378.7199 - mae: 7.6564\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 377.7584 - mae: 7.4595\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 379.6429 - mae: 7.6086\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 984us/step - loss: 377.9619 - mae: 7.5976\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 378.5331 - mae: 7.5786\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 378.5970 - mae: 7.5144\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 934us/step - loss: 379.2151 - mae: 7.5799\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 966us/step - loss: 378.9510 - mae: 7.5615\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 916us/step - loss: 379.2618 - mae: 7.5907\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 868us/step - loss: 378.4436 - mae: 7.6117\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 378.2879 - mae: 7.5963\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 885us/step - loss: 378.7312 - mae: 7.5597\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 377.7298 - mae: 7.5218\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 380.7398 - mae: 7.6607\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 379.6986 - mae: 7.5502\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 868us/step - loss: 377.9595 - mae: 7.5707\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 895us/step - loss: 377.7106 - mae: 7.5458\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 876us/step - loss: 377.7783 - mae: 7.5080\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 869us/step - loss: 378.8182 - mae: 7.6074\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 885us/step - loss: 382.0171 - mae: 7.6418\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 879us/step - loss: 378.7294 - mae: 7.5996\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 952us/step - loss: 377.8681 - mae: 7.5487\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 887us/step - loss: 377.9956 - mae: 7.5359\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 876us/step - loss: 378.1833 - mae: 7.5617\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 875us/step - loss: 378.5271 - mae: 7.5521\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 884us/step - loss: 379.0631 - mae: 7.6064\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 876us/step - loss: 378.2524 - mae: 7.5716\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 887us/step - loss: 379.0060 - mae: 7.5851\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 377.8107 - mae: 7.5553\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 377.5770 - mae: 7.4997\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 381.5379 - mae: 7.7044\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 930us/step - loss: 377.8543 - mae: 7.5796\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 915us/step - loss: 377.6521 - mae: 7.5113\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 875us/step - loss: 380.0593 - mae: 7.6442\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 877us/step - loss: 378.5745 - mae: 7.5415\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 861us/step - loss: 378.3889 - mae: 7.6150\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 867us/step - loss: 379.3485 - mae: 7.5644\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 874us/step - loss: 377.8740 - mae: 7.6051\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 863us/step - loss: 378.4916 - mae: 7.5837\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 862us/step - loss: 378.8082 - mae: 7.5409\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 859us/step - loss: 377.9610 - mae: 7.5356\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 378.9103 - mae: 7.6429\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 378.1864 - mae: 7.6147\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 378.9264 - mae: 7.5692\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 378.9929 - mae: 7.5846\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 873us/step - loss: 378.6301 - mae: 7.5565\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 378.6374 - mae: 7.5872\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 853us/step - loss: 378.8817 - mae: 7.5690\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 379.3548 - mae: 7.5688\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 864us/step - loss: 380.2007 - mae: 7.7034\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 377.7880 - mae: 7.5672\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 856us/step - loss: 379.5379 - mae: 7.5486\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 378.6427 - mae: 7.5917\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 963us/step - loss: 378.3844 - mae: 7.5735\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 378.4113 - mae: 7.5576\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 859us/step - loss: 379.0826 - mae: 7.5965\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 378.2591 - mae: 7.5428\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 849us/step - loss: 378.5198 - mae: 7.6189\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 378.2891 - mae: 7.5702\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 861us/step - loss: 378.7613 - mae: 7.6033\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 377.8299 - mae: 7.5816\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(weather_to_characteristics.astype(float), weather_to_characteristics_y.astype(float), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 473us/step\n",
      "(1164, 1) (1164, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming y_pred and y_test are the predicted and true values respectively\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y, weather_to_characteristics_y)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Absolute Error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mae)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    141\u001b[0m     {\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    205\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    208\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:110\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    112\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m         )\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    117\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=20)"
     ]
    }
   ],
   "source": [
    "y=model_1.predict(weather_to_characteristics.astype(float))\n",
    "print(y.shape, weather_to_characteristics_y.shape)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming y_pred and y_test are the predicted and true values respectively\n",
    "mae = mean_absolute_error(y, weather_to_characteristics_y)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted target values, respectively\n",
    "r2 = r2_score(y, weather_to_characteristics_y)\n",
    "\n",
    "print(\"R-squared (R2) Score:\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
